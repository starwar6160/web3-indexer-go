package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log/slog"
	"math/big"
	"net/http"
	"time"

	"web3-indexer-go/internal/engine"

	"github.com/jmoiron/sqlx"
)

// REST Models
type Block struct {
	ProcessedAt string `db:"processed_at" json:"processed_at"`
	Number      string `db:"number" json:"number"`
	Hash        string `db:"hash" json:"hash"`
	ParentHash  string `db:"parent_hash" json:"parent_hash"`
	Timestamp   string `db:"timestamp" json:"timestamp"`
}

type Transfer struct {
	ID           int    `db:"id" json:"id"`
	BlockNumber  string `db:"block_number" json:"block_number"`
	TxHash       string `db:"tx_hash" json:"tx_hash"`
	LogIndex     int    `db:"log_index" json:"log_index"`
	FromAddress  string `db:"from_address" json:"from_address"`
	ToAddress    string `db:"to_address" json:"to_address"`
	Amount       string `db:"amount" json:"amount"`
	TokenAddress string `db:"token_address" json:"token_address"`
	Symbol       string `db:"symbol" json:"symbol"`
	Type         string `db:"activity_type" json:"type"`
}

func handleGetBlocks(w http.ResponseWriter, r *http.Request, db *sqlx.DB) {
	type dbBlock struct {
		ProcessedAt time.Time `db:"processed_at"`
		Number      string    `db:"number"`
		Hash        string    `db:"hash"`
		ParentHash  string    `db:"parent_hash"`
		Timestamp   string    `db:"timestamp"`
	}
	var rawBlocks []dbBlock
	err := db.SelectContext(r.Context(), &rawBlocks, `SELECT number, hash, parent_hash, timestamp, processed_at FROM blocks ORDER BY number DESC LIMIT 10`)
	if err != nil {
		http.Error(w, "Failed to retrieve blocks", 500)
		return
	}

	blocks := make([]Block, len(rawBlocks))
	for i, b := range rawBlocks {
		blocks[i] = Block{
			Number:      b.Number,
			Hash:        b.Hash,
			ParentHash:  b.ParentHash,
			Timestamp:   b.Timestamp,
			ProcessedAt: b.ProcessedAt.Format("15:04:05.000"),
		}
	}

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(map[string]interface{}{"blocks": blocks}); err != nil {
		slog.Error("failed_to_encode_blocks", "err", err)
	}
}

func handleGetTransfers(w http.ResponseWriter, r *http.Request, db *sqlx.DB) {
	var transfers []Transfer
	err := db.SelectContext(r.Context(), &transfers, "SELECT id, block_number, tx_hash, log_index, from_address, to_address, amount, token_address, symbol, activity_type FROM transfers ORDER BY block_number DESC, log_index DESC LIMIT 10")
	if err != nil {
		http.Error(w, "Failed to retrieve transfers", 500)
		return
	}
	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(map[string]interface{}{"transfers": transfers}); err != nil {
		slog.Error("failed_to_encode_transfers", "err", err)
	}
}

func handleGetTransfersFromHotBuffer(w http.ResponseWriter, processor *engine.Processor) {
	hotTransfers := processor.GetHotBuffer().GetLatest(10)
	apiTransfers := make([]Transfer, len(hotTransfers))
	for i, t := range hotTransfers {
		// #nosec G115 - LogIndex is within safe range for int
		apiTransfers[i] = Transfer{
			BlockNumber:  t.BlockNumber.String(),
			TxHash:       t.TxHash,
			LogIndex:     int(t.LogIndex),
			FromAddress:  t.From,
			ToAddress:    t.To,
			Amount:       t.Amount.String(),
			TokenAddress: t.TokenAddress,
			Symbol:       t.Symbol,
			Type:         t.Type,
		}
	}
	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(map[string]interface{}{
		"transfers": apiTransfers,
		"source":    "hot_buffer",
	}); err != nil {
		slog.Error("failed_to_encode_hot_transfers", "err", err)
	}
}

func handleInitialStatus(w http.ResponseWriter, title string) {
	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(map[string]interface{}{
		"state": "initializing",
		"title": title,
		"msg":   "Database or RPC not ready yet",
	}); err != nil {
		slog.Error("failed_to_encode_init_status", "err", err)
	}
}

func handleGetStatus(w http.ResponseWriter, r *http.Request, db *sqlx.DB, rpcPool engine.RPCClient, lazyManager *engine.LazyManager, chainID int64, signer *engine.SignerMachine) {
	if lazyManager != nil {
		lazyManager.Trigger()
	}

	ctx := r.Context()

	// ğŸ”¥ åŸå­çŠ¶æ€æ›´æ–°ï¼šLatest (on Chain) = max(Fetcher_Current, RPC_Latest)
	// è§£å†³æŒ‡æ ‡æ›´æ–°æ»åé—®é¢˜ï¼šDashboard æ˜¾ç¤ºçš„é«˜åº¦å¯èƒ½è½åäº TailFollow å®é™…è°ƒåº¦çš„é«˜åº¦
	//
	// ç­–ç•¥ï¼š
	// 1. ä¼˜å…ˆä½¿ç”¨ HeightOracleï¼ˆTailFollow æ¨é€ï¼‰
	// 2. å¦‚æœä¸º 0ï¼ˆå†·å¯åŠ¨ï¼‰ï¼Œåˆ™ä» RPC æ‹‰å–
	// 3. Anvil ç¯å¢ƒï¼šæ¯æ¬¡å¼ºåˆ¶åˆ·æ–°ï¼ˆæ¶ˆé™¤ç¼“å­˜ï¼‰
	snap := engine.GetHeightOracle().Snapshot()
	latestChainInt64 := snap.ChainHead

	// å†·å¯åŠ¨æˆ– Anvil ç¯å¢ƒï¼šå¼ºåˆ¶ä» RPC è·å–æœ€æ–°é«˜åº¦
	if latestChainInt64 == 0 || chainID == 31337 {
		if tip, err := rpcPool.GetLatestBlockNumber(ctx); err == nil && tip != nil {
			rpcHeight := tip.Int64()
			// åªæ›´æ–° RPC é«˜åº¦æ›´é«˜æ—¶ï¼ˆé¿å…å›é€€ï¼‰
			if rpcHeight > latestChainInt64 {
				engine.GetHeightOracle().SetChainHead(rpcHeight)
				latestChainInt64 = rpcHeight
			}
		}
	}

	// Use HeightOracle snapshot as the single source of truth for all height
	// numbers. This eliminates the race where /api/status calls GetLatestBlockNumber
	// on a potentially lagging RPC node while TailFollow has already advanced
	// further, producing "Synced > On-Chain" phantom readings.
	//
	// HeightOracle.ChainHead() is written exclusively by TailFollow (every 500ms),
	// which is the most authoritative and up-to-date source.
	// HeightOracle.IndexedHead() is written by Processor after each checkpoint commit.
	// Re-read snapshot after potential update
	snap = engine.GetHeightOracle().Snapshot()
	latestChainInt64 = snap.ChainHead
	latestIndexedBlockInt64 := snap.IndexedHead
	latestIndexedBlock := fmt.Sprintf("%d", latestIndexedBlockInt64)

	// If HeightOracle hasn't been populated yet (cold start before first TailFollow
	// tick), fall back to a live RPC call exactly once.
	if latestChainInt64 == 0 {
		if tip, err := rpcPool.GetLatestBlockNumber(ctx); err == nil && tip != nil {
			latestChainInt64 = tip.Int64()
			engine.GetHeightOracle().SetChainHead(latestChainInt64)
		}
	}
	if latestIndexedBlockInt64 == 0 {
		latestIndexedBlock = getLatestIndexedBlock(ctx, db)
		latestIndexedBlockInt64 = parseBlockNumber(latestIndexedBlock)
	}

	// ğŸ”¥ åŸå­ Lag è®¡ç®—ï¼šé¿å…ä½¿ç”¨è¿‡æ—¶çš„å¿«ç…§å€¼
	// å…¬å¼ï¼šSyncLag = max(0, LatestChain - LatestIndexed)
	// ä¸ä½¿ç”¨ä»»ä½•ä¸­é—´å˜é‡ï¼Œç›´æ¥ä»æ•°æ®åº“å®æ—¶æŸ¥è¯¢
	var totalBlocksInDB int64
	err := db.GetContext(ctx, &totalBlocksInDB, "SELECT COUNT(*) FROM blocks")
	if err != nil {
		totalBlocksInDB = 0
	}

	// å®æ—¶è®¡ç®— SyncLagï¼ˆåŸå­æ“ä½œï¼‰
	rawSyncLag := latestChainInt64 - latestIndexedBlockInt64
	var syncLag int64
	var driftBlocks int64
	var isTimeTravel bool

	if rawSyncLag < 0 {
		// æ—¶é—´æ—…è¡Œï¼šIndexed > Chainï¼ˆå¯èƒ½å› ä¸º RPC èŠ‚ç‚¹æ»åï¼‰
		driftBlocks = -rawSyncLag
		syncLag = 0
		isTimeTravel = driftBlocks > engine.GetHeightOracle().DriftTolerance
	} else {
		// æ­£å¸¸æƒ…å†µ
		syncLag = rawSyncLag
		driftBlocks = 0
		isTimeTravel = false
	}

	e2eLatencyDisplay, e2eLatencySeconds := calculateLatency(ctx, db, latestChainInt64, latestIndexedBlockInt64, latestIndexedBlock)

	// ğŸš€ Local/Anvil/Replay Smoothing: Ignore astronomical latency
	if chainID == 31337 && e2eLatencySeconds > 3600 {
		e2eLatencyDisplay = "0.00s (Replay)"
		e2eLatencySeconds = 0
	}

	status := make(map[string]interface{})
	status["version"] = Version
	status["state"] = engine.ModeActive
	status["latest_block"] = fmt.Sprintf("%d", latestChainInt64)
	status["latest_indexed"] = latestIndexedBlock
	status["sync_lag"] = syncLag
	// time_travel: indexer has processed blocks beyond the reported chain head.
	// Caused by RPC node lag or stale HeightOracle (not a data integrity issue
	// unless drift_blocks > drift_tolerance).
	status["time_travel"] = isTimeTravel
	status["drift_blocks"] = driftBlocks
	// ğŸ”¥ å®æ—¶æŸ¥è¯¢æ€»å—æ•°ï¼ˆé¿å…ä½¿ç”¨è¿‡æ—¶çš„ç¼“å­˜å€¼ï¼‰
	status["total_blocks"] = totalBlocksInDB
	status["total_transfers"] = getCount(ctx, db, "SELECT COUNT(*) FROM transfers")
	status["tps"] = calculateTPS(ctx, db)
	status["is_catching_up"] = syncLag > 10
	status["is_healthy"] = rpcPool.GetHealthyNodeCount() > 0
	status["rpc_nodes"] = map[string]int{
		"healthy": rpcPool.GetHealthyNodeCount(),
		"total":   rpcPool.GetTotalNodeCount(),
	}
	status["e2e_latency_seconds"] = e2eLatencySeconds
	status["e2e_latency_display"] = e2eLatencyDisplay

	// ğŸ¯ åŒæ­¥è¿›åº¦ç™¾åˆ†æ¯”è®¡ç®—ï¼ˆåŸå­æ“ä½œï¼‰
	// è§„åˆ™ï¼šåˆ†æ¯å¿…é¡» >= åˆ†å­ï¼Œå¦åˆ™ç™¾åˆ†æ¯”æ— æ„ä¹‰ã€‚
	// å½“ indexedHead > chainHeadï¼ˆæ—¶é—´æ—…è¡Œï¼‰æ—¶ï¼Œè¿›åº¦è§†ä¸º 100%ï¼ˆå·²è¿½ä¸Šï¼‰ã€‚
	// å½“ chainHead == 0ï¼ˆå†·å¯åŠ¨ï¼‰æ—¶ï¼Œè¿›åº¦ä¸º 0%ã€‚
	syncProgressPercent := 0.0
	if latestChainInt64 > 0 && latestIndexedBlockInt64 > 0 {
		if latestIndexedBlockInt64 >= latestChainInt64 {
			syncProgressPercent = 100.0
		} else {
			syncProgressPercent = float64(latestIndexedBlockInt64) / float64(latestChainInt64) * 100.0
		}
	}
	status["sync_progress_percent"] = syncProgressPercent

	// f5: Standby æ¨¡å¼ä¸‹æ ‡è®°æ•°æ®ä¸º staleï¼Œè®©å‰ç«¯åŒºåˆ†ç¼“å­˜æ•°æ®å’Œå®æ—¶æ•°æ®ã€‚
	// oracle_age_ms > 5000 è¡¨ç¤º TailFollow å·²è¶…è¿‡ 5 ç§’æœªæ›´æ–°é“¾é«˜åº¦ã€‚
	oracleAgeMs := time.Since(snap.UpdatedAt).Milliseconds()
	if snap.UpdatedAt.IsZero() {
		oracleAgeMs = -1 // å°šæœªåˆå§‹åŒ–
	}
	status["height_oracle_age_ms"] = oracleAgeMs
	status["data_is_stale"] = oracleAgeMs < 0 || oracleAgeMs > 5000

	if lazyManager != nil {
		lazyStatus := lazyManager.GetStatus()
		if mode, ok := lazyStatus["mode"].(string); ok {
			status["state"] = mode
			status["lazy_indexer"] = lazyStatus
			// Standby æ¨¡å¼ä¸‹æ•°æ®å¿…ç„¶æ˜¯ stale çš„
			if mode == engine.ModeSleep {
				status["data_is_stale"] = true
			}
		}
	}

	if signer != nil {
		if signed, err := signer.Sign("status", status); err == nil {
			w.Header().Set("X-Payload-Signature", signed.Signature)
			w.Header().Set("X-Signer-ID", signed.SignerID)
			w.Header().Set("X-Public-Key", signed.PubKey)
		}
	}

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(status); err != nil {
		slog.Error("failed_to_encode_status", "err", err)
	}
}

func getLatestIndexedBlock(ctx context.Context, db *sqlx.DB) string {
	var latest string
	if err := db.GetContext(ctx, &latest, "SELECT COALESCE(MAX(number), '0') FROM blocks"); err != nil {
		return "0"
	}
	return latest
}

func getCount(ctx context.Context, db *sqlx.DB, query string) int64 {
	var count int64
	if err := db.GetContext(ctx, &count, query); err != nil {
		return 0
	}
	return count
}

func parseBlockNumber(s string) int64 {
	if s == "" || s == "0" {
		return 0
	}
	if parsed, ok := new(big.Int).SetString(s, 10); ok {
		return parsed.Int64()
	}
	return 0
}

func calculateLatency(ctx context.Context, db *sqlx.DB, latestChain, latestIndexed int64, latestIndexedStr string) (string, float64) {
	if latestChain <= 0 || latestIndexed <= 0 {
		return "0s", 0
	}
	syncLag := latestChain - latestIndexed
	if syncLag > 100 {
		estLatency := float64(syncLag) * 12
		return fmt.Sprintf("Catching up... (%d blocks behind)", syncLag), estLatency
	}
	var processedAt time.Time
	err := db.GetContext(ctx, &processedAt, "SELECT processed_at FROM blocks WHERE number = $1", latestIndexedStr)
	if err == nil && !processedAt.IsZero() {
		latency := time.Since(processedAt).Seconds()
		return fmt.Sprintf("%.2fs", latency), latency
	}
	return fmt.Sprintf("%.2fs", float64(syncLag)*12), float64(syncLag) * 12
}

func calculateTPS(_ context.Context, _ *sqlx.DB) float64 {
	return engine.GetMetrics().GetWindowTPS()
}
