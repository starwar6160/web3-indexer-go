package engine

import (
	"context"
	"fmt"
	"log/slog"
	"math/big"
	"runtime"
	"sync"
	"time"
)

// ğŸ”¥ åŠ¨æ€èƒŒå‹ç®¡ç†å™¨ (Dynamic Backpressure Manager)
// é’ˆå¯¹ 128G RAM ç¯å¢ƒä¼˜åŒ–ï¼Œè‡ªåŠ¨è°ƒæ•´æ°´ä½çº¿å’Œè°ƒåº¦ç­–ç•¥

type BackpressureManager struct {
	// é…ç½®
	maxJobsCapacity    int
	maxResultsCapacity int
	maxSeqBuffer       int

	// åŠ¨æ€æ°´ä½çº¿ï¼ˆè‡ªé€‚åº”ï¼‰
	jobsWatermark      int
	resultsWatermark   int
	seqBufferWatermark int

	// æŒ‡æ•°é€€é¿çŠ¶æ€
	lastBlockedTime time.Time
	backoffLevel    int
	backoffMu       sync.Mutex

	// ç»Ÿè®¡
	totalBlockedCount uint64
}

// NewBackpressureManager åˆ›å»ºèƒŒå‹ç®¡ç†å™¨
func NewBackpressureManager() *BackpressureManager {
	// ğŸ”¥ æ¨ªæ»¨å®éªŒå®¤ï¼šæ ¹æ®å†…å­˜å¤§å°åŠ¨æ€è®¡ç®—å®¹é‡
	// å‡è®¾ç³»ç»Ÿå†…å­˜ 128Gï¼Œæˆ‘ä»¬å¯ä»¥æ¿€è¿›åœ°åˆ†é…ç¼“å†²åŒº
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)

	// æ¯ GB å†…å­˜åˆ†é… 1000 ä¸ªç¼“å†²åŒºæ§½ä½
	memGB := memStats.Sys / (1024 * 1024 * 1024)
	maxResultsCapacity := int(memGB * 1000) // 128G â†’ 128,000

	// ä¿å®ˆä¸€ç‚¹ï¼Œè®¾ç½®ä¸Šé™ä¸º 100,000
	if maxResultsCapacity > 100000 {
		maxResultsCapacity = 100000
	}
	if maxResultsCapacity < 5000 {
		maxResultsCapacity = 5000 // æœ€å°å€¼
	}

	maxJobsCapacity := maxResultsCapacity / 5 // Jobs é˜Ÿåˆ—å°ä¸€äº›
	maxSeqBuffer := 10000                     // Sequencer buffer

	return &BackpressureManager{
		maxJobsCapacity:    maxJobsCapacity,
		maxResultsCapacity: maxResultsCapacity,
		maxSeqBuffer:       maxSeqBuffer,
		jobsWatermark:      maxJobsCapacity * 80 / 100,
		resultsWatermark:   maxResultsCapacity * 80 / 100,
		seqBufferWatermark: 800, // å›ºå®š 800 é˜ˆå€¼
		backoffLevel:       0,
	}
}

// CheckSchedule æ£€æŸ¥æ˜¯å¦å¯ä»¥è°ƒåº¦ï¼ˆå¸¦æŒ‡æ•°é€€é¿ï¼‰
func (bpm *BackpressureManager) CheckSchedule(jobsDepth, resultsDepth, seqBufferSize int) error {
	// ğŸ”¥ 1. åŸºç¡€æ°´ä½çº¿æ£€æŸ¥
	if jobsDepth > bpm.jobsWatermark {
		return bpm.blockWithError("jobs", jobsDepth, bpm.maxJobsCapacity, bpm.jobsWatermark, resultsDepth)
	}

	if resultsDepth > bpm.resultsWatermark {
		return bpm.blockWithError("results", resultsDepth, bpm.maxResultsCapacity, bpm.resultsWatermark, jobsDepth)
	}

	if seqBufferSize > bpm.seqBufferWatermark {
		return bpm.blockWithError("sequencer", seqBufferSize, bpm.maxSeqBuffer, bpm.seqBufferWatermark, resultsDepth)
	}

	// ğŸ”¥ 2. æŒ‡æ•°é€€é¿æ£€æŸ¥ï¼ˆé˜²æ­¢ç–¯ç‹‚é‡è¯•ï¼‰
	bpm.backoffMu.Lock()
	defer bpm.backoffMu.Unlock()

	if bpm.backoffLevel > 0 {
		elapsed := time.Since(bpm.lastBlockedTime)
		backoffDuration := time.Duration(1<<bpm.backoffLevel) * 100 * time.Millisecond

		if elapsed < backoffDuration {
			return fmt.Errorf("backoff: waiting %v (level %d)", backoffDuration-elapsed, bpm.backoffLevel)
		}

		// é€€é¿æ—¶é—´åˆ°äº†ï¼Œé‡ç½®é€€é¿çº§åˆ«
		bpm.backoffLevel = 0
		slog.Info("âœ… [Backpressure] Backoff reset", "level", 0)
	}

	return nil
}

// blockWithError è®°å½•é˜»å¡å¹¶è§¦å‘æŒ‡æ•°é€€é¿
func (bpm *BackpressureManager) blockWithError(component string, depth, capacity, watermark, otherDepth int) error {
	bpm.backoffMu.Lock()
	defer bpm.backoffMu.Unlock()

	bpm.totalBlockedCount++
	bpm.lastBlockedTime = time.Now()

	// å¢åŠ é€€é¿çº§åˆ«ï¼ˆæœ€å¤§ 5 çº§ï¼š100ms â†’ 3.2sï¼‰
	if bpm.backoffLevel < 5 {
		bpm.backoffLevel++
	}

	backoffDuration := time.Duration(1<<bpm.backoffLevel) * 100 * time.Millisecond

	slog.Warn("ğŸš« [Backpressure] SCHEDULE_BLOCKED",
		slog.String("component", component),
		slog.Int("depth", depth),
		slog.Int("watermark", watermark),
		slog.Int("capacity", capacity),
		slog.Int("other_depth", otherDepth),
		slog.Int("backoff_level", bpm.backoffLevel),
		slog.Duration("backoff_duration", backoffDuration),
		slog.Uint64("total_blocked", bpm.totalBlockedCount))

	return fmt.Errorf("%s queue backpressure: depth=%d/%d (backoff level %d)", component, depth, capacity, bpm.backoffLevel)
}

// ResetBackoff é‡ç½®é€€é¿çº§åˆ«ï¼ˆæˆåŠŸè°ƒåº¦åè°ƒç”¨ï¼‰
func (bpm *BackpressureManager) ResetBackoff() {
	bpm.backoffMu.Lock()
	defer bpm.backoffMu.Unlock()

	if bpm.backoffLevel > 0 {
		slog.Info("âœ… [Backpressure] Backoff cleared", "previous_level", bpm.backoffLevel)
		bpm.backoffLevel = 0
	}
}

// GetCapacity è·å–å½“å‰å®¹é‡é…ç½®
func (bpm *BackpressureManager) GetCapacity() (maxJobs, maxResults, maxSeq int) {
	return bpm.maxJobsCapacity, bpm.maxResultsCapacity, bpm.maxSeqBuffer
}

// GetWatermarks è·å–å½“å‰æ°´ä½çº¿é…ç½®
func (bpm *BackpressureManager) GetWatermarks() (jobs, results, seq int) {
	return bpm.jobsWatermark, bpm.resultsWatermark, bpm.seqBufferWatermark
}

// ğŸ”¥ ä»»åŠ¡åˆå¹¶ï¼ˆTask Mergingï¼‰ï¼šé˜²æ­¢é‡å¤è°ƒåº¦

type MergeKey struct {
	Start string
	End   string
}

type ScheduleMerge struct {
	mu          sync.Mutex
	pending     map[MergeKey]time.Time // æœ€åè°ƒåº¦æ—¶é—´
	mergeWindow time.Duration          // åˆå¹¶çª—å£ï¼ˆåŒä¸€èŒƒå›´å†…çš„è°ƒåº¦ä¼šè¢«åˆå¹¶ï¼‰
}

func NewScheduleMerge(mergeWindow time.Duration) *ScheduleMerge {
	return &ScheduleMerge{
		pending:     make(map[MergeKey]time.Time),
		mergeWindow: mergeWindow,
	}
}

// ShouldSchedule æ£€æŸ¥æ˜¯å¦åº”è¯¥è°ƒåº¦ï¼ˆå¸¦åˆå¹¶é€»è¾‘ï¼‰
func (sm *ScheduleMerge) ShouldSchedule(start, end *big.Int) bool {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	key := MergeKey{
		Start: start.String(),
		End:   end.String(),
	}

	now := time.Now()
	if lastTime, exists := sm.pending[key]; exists {
		// å¦‚æœåœ¨åˆå¹¶çª—å£å†…ï¼Œè·³è¿‡é‡å¤è°ƒåº¦
		if now.Sub(lastTime) < sm.mergeWindow {
			slog.Debug("ğŸ”„ [Merge] Schedule skipped (merged)",
				slog.String("range", fmt.Sprintf("%s-%s", start.String(), end.String())),
				slog.Duration("since_last", now.Sub(lastTime)))
			return false
		}
	}

	// æ›´æ–°æœ€åè°ƒåº¦æ—¶é—´
	sm.pending[key] = now
	return true
}

// Cleanup æ¸…ç†è¿‡æœŸçš„åˆå¹¶è®°å½•
func (sm *ScheduleMerge) Cleanup() {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	now := time.Now()
	for key, lastTime := range sm.pending {
		if now.Sub(lastTime) > 10*time.Minute {
			delete(sm.pending, key)
		}
	}
}

// ğŸ”¥ Fetcher é›†æˆï¼šä½¿ç”¨åŠ¨æ€èƒŒå‹ç®¡ç†å™¨

// FetcherWithDynamicBackpressure æ”¯æŒåŠ¨æ€èƒŒå‹çš„ Fetcher
type FetcherWithDynamicBackpressure struct {
	*Fetcher

	backpressureMgr *BackpressureManager
	scheduleMerge   *ScheduleMerge
}

func NewFetcherWithDynamicBackpressure(fetcher *Fetcher) *FetcherWithDynamicBackpressure {
	return &FetcherWithDynamicBackpressure{
		Fetcher:         fetcher,
		backpressureMgr: NewBackpressureManager(),
		scheduleMerge:   NewScheduleMerge(5 * time.Second), // 5ç§’åˆå¹¶çª—å£
	}
}

// ScheduleDynamic ä½¿ç”¨åŠ¨æ€èƒŒå‹æ£€æµ‹çš„è°ƒåº¦æ–¹æ³•
func (f *FetcherWithDynamicBackpressure) ScheduleDynamic(ctx context.Context, start, end *big.Int) error {
	// 1. ä»»åŠ¡åˆå¹¶æ£€æŸ¥
	if !f.scheduleMerge.ShouldSchedule(start, end) {
		return nil // å·²åˆå¹¶ï¼Œè·³è¿‡
	}

	// 2. åŠ¨æ€èƒŒå‹æ£€æµ‹
	jobsDepth := len(f.jobs)
	resultsDepth := len(f.Results)
	seqBufferSize := 0
	if f.sequencer != nil {
		seqBufferSize = f.sequencer.GetBufferSize()
	}

	if err := f.backpressureMgr.CheckSchedule(jobsDepth, resultsDepth, seqBufferSize); err != nil {
		return err
	}

	// 3. æ‰§è¡Œè°ƒåº¦
	if err := f.Schedule(ctx, start, end); err != nil {
		return err
	}

	// 4. æˆåŠŸè°ƒåº¦åé‡ç½®é€€é¿
	f.backpressureMgr.ResetBackoff()

	return nil
}

// GetBackpressureStats è·å–èƒŒå‹ç»Ÿè®¡ä¿¡æ¯
func (f *FetcherWithDynamicBackpressure) GetBackpressureStats() map[string]interface{} {
	maxJobs, maxResults, maxSeq := f.backpressureMgr.GetCapacity()
	jobsWm, resultsWm, seqWm := f.backpressureMgr.GetWatermarks()

	return map[string]interface{}{
		"max_jobs_capacity":     maxJobs,
		"max_results_capacity":  maxResults,
		"max_seq_buffer":        maxSeq,
		"jobs_watermark":        jobsWm,
		"results_watermark":     resultsWm,
		"seq_watermark":         seqWm,
		"current_jobs_depth":    len(f.jobs),
		"current_results_depth": len(f.Results),
		"current_seq_buffer":    f.sequencer.GetBufferSize(),
		"total_blocked_count":   f.backpressureMgr.totalBlockedCount,
	}
}
