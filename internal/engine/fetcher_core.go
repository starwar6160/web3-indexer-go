package engine

import (
	"context"
	"log/slog"
	"math/big"
	"os"
	"strconv"
	"sync"

	"web3-indexer-go/internal/limiter"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"golang.org/x/time/rate"
)

type BlockData struct {
	Number   *big.Int
	RangeEnd *big.Int // Used for range processing (if applicable)
	Block    *types.Block
	Err      error
	Logs     []types.Log
}

type FetchJob struct {
	Start *big.Int
	End   *big.Int
}

type Fetcher struct {
	pool        RPCClient // RPCå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒMockå’ŒçœŸå®å®ç°
	concurrency int
	jobs        chan FetchJob
	Results     chan BlockData
	limiter     *rate.Limiter // é€Ÿç‡é™åˆ¶å™¨
	throughput  *rate.Limiter // ğŸš€ Throughput limiter for visual/speed control
	bpsLimiter  *rate.Limiter // ğŸš€ ğŸ”¥ æ–°å¢ï¼šå—çº§åˆ«èŠ‚æ‹å™¨ (Pacemaker)
	stopCh      chan struct{} // ç”¨äºåœæ­¢è°ƒåº¦
	stopOnce    sync.Once     // ç¡®ä¿åªåœæ­¢ä¸€æ¬¡
	metrics     *Metrics      // Prometheus metrics

	// Pause/Resume æœºåˆ¶ï¼šç”¨ sync.Cond æ›¿ä»£ channel é¿å…ç«æ€
	pauseMu   sync.Mutex
	pauseCond *sync.Cond
	paused    bool

	// Watched addresses for contract monitoring
	watchedAddresses []common.Address

	headerOnlyMode bool          // ä½æˆæœ¬æ¨¡å¼ï¼šä»…è·å–åŒºå—å¤´ï¼Œä¸è·å–Logs
	recorder       *DataRecorder // ğŸ’¾ åŸå§‹æ•°æ®å½•åˆ¶å™¨

	// ğŸ”¥ æ¨ªæ»¨å®éªŒå®¤ï¼šèƒŒå‹æ£€æµ‹
	sequencer *Sequencer // Sequencer å¼•ç”¨ï¼ˆç”¨äºæ£€æµ‹ buffer æ·±åº¦ï¼‰
}

// ğŸ”¥ QueueDepth è¿”å›é˜Ÿåˆ—æ·±åº¦ï¼ˆç”¨äºä¸Šæ¸¸èƒŒå‹æ£€æµ‹ï¼‰
func (f *Fetcher) QueueDepth() int {
	return len(f.jobs)
}

// ğŸ”¥ ResultsDepth è¿”å›ç»“æœé€šé“æ·±åº¦ï¼ˆç”¨äºä¸Šæ¸¸èƒŒå‹æ£€æµ‹ï¼‰
func (f *Fetcher) ResultsDepth() int {
	return len(f.Results)
}

// ğŸ”¥ ClearJobs æ¸…ç©ºä»»åŠ¡é˜Ÿåˆ— (ç”¨äº Ephemeral Mode é‡ç½®)
func (f *Fetcher) ClearJobs() {
	count := 0
	for {
		select {
		case <-f.jobs:
			count++
		default:
			if count > 0 {
				slog.Warn("ğŸŒ€ [Fetcher] Jobs queue purged", "cleared", count)
			}
			return
		}
	}
}

// ğŸ”¥ SetSequencer è®¾ç½® Sequencer å¼•ç”¨ï¼ˆç”¨äºèƒŒå‹æ£€æµ‹ï¼‰
func (f *Fetcher) SetSequencer(seq *Sequencer) {
	f.sequencer = seq
}

// SetHeaderOnlyMode enables/disables low-cost mode
func (f *Fetcher) SetHeaderOnlyMode(enabled bool) {
	f.headerOnlyMode = enabled
	if enabled {
		Logger.Info("fetcher_switched_to_low_cost_header_only_mode")
	} else {
		Logger.Info("fetcher_switched_to_full_data_mode")
	}
}

func NewFetcher(pool RPCClient, concurrency int) *Fetcher {
	// å½»åº•å…³é—­é™é€Ÿ
	limiter := rate.NewLimiter(rate.Inf, 0)

	// ğŸ’¾ åˆå§‹åŒ–å½•åˆ¶å™¨
	recorder, err := NewDataRecorder("")
	if err != nil {
		slog.Warn("failed_to_initialize_recorder", "err", err)
	}

	f := &Fetcher{
		pool:        pool,
		concurrency: concurrency,
		// ğŸ”¥ æ¨ªæ»¨å®éªŒå®¤ï¼šJobs channel ä¹Ÿæ‰©å®¹ (concurrency * 10)
		jobs: make(chan FetchJob, concurrency*10),
		// ğŸ”¥ 16G RAM è°ƒä¼˜ï¼šæå‡è‡³å¯é…ç½®å®¹é‡ï¼Œç»™äºˆæ¶ˆè´¹ç«¯æ›´å¤šç¼“å†²ç©ºé—´
		Results:  make(chan BlockData, getFetcherResultsChannelSize()),
		limiter:  limiter,
		recorder: recorder,
		stopCh:   make(chan struct{}),
		paused:   false,
		metrics:  GetMetrics(),
	}
	f.pauseCond = sync.NewCond(&f.pauseMu)
	return f
}

// getFetcherResultsChannelSize ä»ç¯å¢ƒå˜é‡è¯»å– Results channel å®¹é‡
// é»˜è®¤ 15000ï¼Œé€‚åˆ 16G RAM ç¯å¢ƒ
func getFetcherResultsChannelSize() int {
	const defaultCapacity = 15000
	const envKey = "FETCHER_RESULTS_SIZE"

	val := os.Getenv(envKey)
	if val == "" {
		return defaultCapacity
	}

	size, err := strconv.Atoi(val)
	if err != nil || size <= 0 {
		slog.Warn("âš ï¸ [Fetcher] Invalid FETCHER_RESULTS_SIZE, using default",
			"value", val,
			"default", defaultCapacity,
			"error", err)
		return defaultCapacity
	}

	slog.Info("ğŸ“Š [Fetcher] Results channel capacity configured",
		"capacity", size,
		"env", envKey)
	return size
}

func NewFetcherWithLimiter(pool RPCClient, concurrency, rps, burst int) *Fetcher {
	// âœ¨ ä½¿ç”¨å·¥ä¸šçº§é™æµå™¨ï¼ˆè‡ªåŠ¨é™çº§ä¿æŠ¤ï¼‰
	rateLimiter := limiter.NewRateLimiter(rps)
	if burst > 0 {
		rateLimiter.Limiter().SetBurst(burst)
	}

	slog.Info("ğŸ›¡ï¸ Rate limiter initialized",
		"max_rps", rateLimiter.MaxRPS(),
		"concurrency", concurrency,
		"protection", "industrial_grade")

	// ğŸš€ Hard Throttle: default unlimited; caller sets via SetThroughputLimit
	// NOTE: rate.Inf avoids WaitN deadlock when tokens > burst

	throughput := rate.NewLimiter(rate.Inf, 0)

	// ğŸš€ Pacemaker: default unlimited; avoids burst exhaustion on Anvil

	bpsLimiter := rate.NewLimiter(rate.Inf, 0)

	// ğŸ’¾ åˆå§‹åŒ–å½•åˆ¶å™¨ (é»˜è®¤å­˜å‚¨è·¯å¾„)

	recorder, err := NewDataRecorder("")

	if err != nil {

		slog.Warn("failed_to_initialize_recorder", "err", err)

	}

	// ğŸ”¥ 16G RAM è°ƒä¼˜ï¼šæå‡è‡³ 15,000

	f := &Fetcher{

		pool: pool,

		concurrency: concurrency,

		jobs: make(chan FetchJob, concurrency*10), // æ‰©å®¹ 10 å€

		Results: make(chan BlockData, getFetcherResultsChannelSize()), // 16G RAM ç¯å¢ƒé€‚ä¸­é…ç½®ï¼ˆå¯è°ƒï¼‰

		limiter: rateLimiter.Limiter(),

		throughput: throughput,

		bpsLimiter: bpsLimiter,

		recorder: recorder,

		stopCh: make(chan struct{}),

		paused: false,

		metrics: GetMetrics(),
	}
	f.pauseCond = sync.NewCond(&f.pauseMu)
	return f
}

// SetWatchedAddresses sets the contract addresses to monitor for Transfer events
func (f *Fetcher) SetWatchedAddresses(addresses []string) {
	f.watchedAddresses = make([]common.Address, 0, len(addresses))
	for _, addr := range addresses {
		if addr != "" {
			f.watchedAddresses = append(f.watchedAddresses, common.HexToAddress(addr))
		}
	}
}

// SetThroughputLimit updates the target processing speed.
// burst is set equal to tps (minimum 1) so WaitN(ctx, n) never blocks
// permanently when n <= burst. Pass tps <= 0 to disable throttling.
func (f *Fetcher) SetThroughputLimit(tps float64) {
	if tps <= 0 {
		f.throughput = rate.NewLimiter(rate.Inf, 0)
		f.bpsLimiter = rate.NewLimiter(rate.Inf, 0)
		return
	}
	// burst = ceil(tps) so a single WaitN call for up to burst tokens never deadlocks
	burst := int(tps)
	if burst < 1 {
		burst = 1
	}
	f.throughput = rate.NewLimiter(rate.Limit(tps), burst)
	f.bpsLimiter = rate.NewLimiter(rate.Limit(tps*10), burst*10)
}

func (f *Fetcher) Start(ctx context.Context, wg *sync.WaitGroup) {
	Logger.Info("ğŸ“¢ [Fetcher] å¼•æ“åç¨‹å·²è¿›å…¥ Start å‡½æ•°ï¼",
		slog.Int("concurrency", f.concurrency),
	)
	for i := 0; i < f.concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			Logger.Info("ğŸŒ€ [Fetcher] å¾ªç¯æŠ“å–åç¨‹æ­£å¼å¯åŠ¨...",
				slog.Int("worker_id", workerID),
			)
			f.worker(ctx, wg)
		}(i)
	}
}

func (f *Fetcher) worker(ctx context.Context, wg *sync.WaitGroup) {
	defer wg.Done()

	for {
		select {
		case <-ctx.Done():
			return
		case <-f.stopCh:
			return
		case job, ok := <-f.jobs:
			if !ok {
				return
			}

			// ğŸš€ ğŸ”¥ æ–°å¢ï¼šå·¥ä½œè„‰æï¼Œå¸®åŠ©å®šä½ä¸ºä½• Jobs å µå¡
			slog.Debug("ğŸŒ€ [Fetcher] Worker picking up job", "start", job.Start.String(), "end", job.End.String())
			GetOrchestrator().DispatchLog("DEBUG", "ğŸŒ€ Fetcher: Worker processing job", "start", job.Start.String())

			// æ£€æŸ¥æ˜¯å¦æš‚åœï¼ˆReorg å¤„ç†æœŸé—´ï¼‰
			f.pauseMu.Lock()
			for f.paused {
				// ç­‰å¾…æ¢å¤ä¿¡å·ï¼ˆä½¿ç”¨ Cond.Wait é¿å…ç«æ€ï¼‰
				f.pauseCond.Wait()
			}
			f.pauseMu.Unlock()

			// æ£€æŸ¥æ˜¯å¦å·²åœæ­¢ï¼ˆåœ¨ unlock åå†æ£€æŸ¥ï¼‰
			select {
			case <-ctx.Done():
				return
			case <-f.stopCh:
				return
			default:
			}

			// ç­‰å¾…é€Ÿç‡é™åˆ¶ä»¤ç‰Œ
			if err := f.limiter.Wait(ctx); err != nil {
				select {
				case f.Results <- BlockData{Number: job.Start, RangeEnd: job.End, Err: err}:
				case <-ctx.Done():
					return
				case <-f.stopCh:
					return
				}
				continue
			}

			// è·å–èŒƒå›´åŒºå—æ•°æ®
			f.fetchRangeWithLogs(ctx, job.Start, job.End)
		}
	}
}
