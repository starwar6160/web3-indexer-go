package engine

import (
	"context"
	"database/sql"
	"errors"
	"fmt"
	"log"
	"math/big"
	"strings"
	"time"
	"web3-indexer-go/internal/models"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/jmoiron/sqlx"
)

// TransferEventHash is the ERC20 Transfer event signature hash
var TransferEventHash = common.HexToHash("0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef")

// ErrReorgDetected is returned when a blockchain reorganization is detected
var ErrReorgDetected = errors.New("reorg detected: parent hash mismatch")

// ErrReorgNeedRefetch is returned when blocks need to be refetched due to reorg
var ErrReorgNeedRefetch = errors.New("reorg detected: need to refetch from common ancestor")

// ReorgError æºå¸¦è§¦å‘é«˜åº¦çš„ reorg é”™è¯¯ï¼ˆç”¨äºä¸Šå±‚å¤„ç†ï¼‰
type ReorgError struct {
	At *big.Int
}

func (e ReorgError) Error() string {
	return fmt.Sprintf("reorg detected at block %s", e.At.String())
}

// Processor å¤„ç†åŒºå—æ•°æ®å†™å…¥ï¼Œæ”¯æŒæ‰¹é‡å’Œå•æ¡æ¨¡å¼
type Processor struct {
	db     *sqlx.DB
	client RPCClient // RPC client interface for reorg recovery
	metrics *Metrics  // Prometheus metrics
}

// RPCClient defines the interface needed by Processor
type RPCClient interface {
	BlockByNumber(ctx context.Context, number *big.Int) (*types.Block, error)
}

func NewProcessor(db *sqlx.DB, client RPCClient) *Processor {
	return &Processor{db: db, client: client}
}

// ProcessBlockWithRetry å¸¦é‡è¯•çš„åŒºå—å¤„ç†
func (p *Processor) ProcessBlockWithRetry(ctx context.Context, data BlockData, maxRetries int) error {
	var err error
	
	for i := 0; i < maxRetries; i++ {
		err = p.ProcessBlock(ctx, data)
		if err == nil {
			return nil
		}
		
		// æ£€æŸ¥æ˜¯å¦æ˜¯è‡´å‘½é”™è¯¯ï¼ˆä¸éœ€è¦é‡è¯•ï¼‰
		if isFatalError(err) {
			return err
		}
		
		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
		if ctx.Err() != nil {
			return ctx.Err()
		}
		
		// æŒ‡æ•°é€€é¿é‡è¯•ï¼š1s, 2s, 4s
		backoff := time.Duration(1<<i) * time.Second
		LogRPCRetry("ProcessBlock", i+1, err)
		select {
		case <-time.After(backoff):
			// ç»§ç»­é‡è¯•
		case <-ctx.Done():
			return ctx.Err()
		}
	}
	
	return fmt.Errorf("max retries exceeded for block %s: %w", data.Block.Number().String(), err)
}

// isFatalError åˆ¤æ–­é”™è¯¯æ˜¯å¦ä¸éœ€è¦é‡è¯•
func isFatalError(err error) bool {
	if err == nil {
		return false
	}
	
	// Reorg æ£€æµ‹é”™è¯¯éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¸æ˜¯ç®€å•é‡è¯•
	if err == ErrReorgDetected {
		return true
	}
	
	// ReorgError ä¹Ÿæ˜¯è‡´å‘½é”™è¯¯ï¼ˆéœ€è¦ä¸Šå±‚å¤„ç†ï¼‰
	if _, ok := err.(ReorgError); ok {
		return true
	}
	
	// ä¸Šä¸‹æ–‡å–æ¶ˆä¸éœ€è¦é‡è¯•
	if err == context.Canceled || err == context.DeadlineExceeded {
		return true
	}
	
	return false
}

// ProcessBlock å¤„ç†å•ä¸ªåŒºå—ï¼ˆå¿…é¡»åœ¨é¡ºåºä¿è¯ä¸‹è°ƒç”¨ï¼‰
func (p *Processor) ProcessBlock(ctx context.Context, data BlockData) error {
	if data.Err != nil {
		return fmt.Errorf("fetch error: %w", data.Err)
	}
	
	block := data.Block
	blockNum := block.Number()
	start := time.Now()
	log.Printf("Processing block: %s | Hash: %s", blockNum.String(), block.Hash().Hex())

	// å¼€å¯äº‹åŠ¡ (ACID æ ¸å¿ƒ)
	tx, err := p.db.BeginTxx(ctx, &sql.TxOptions{Isolation: sql.LevelSerializable})
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	
	// æ— è®ºæˆåŠŸå¤±è´¥ï¼Œç¡®ä¿ Rollback (Commit å Rollback æ— æ•ˆ)
	defer tx.Rollback()

	// 1. Reorg æ£€æµ‹ (Parent Hash Check)
	var lastBlock models.Block
	err = tx.GetContext(ctx, &lastBlock, 
		"SELECT number, hash, parent_hash, timestamp FROM blocks WHERE number = $1", 
		new(big.Int).Sub(blockNum, big.NewInt(1)).String())
	
	if err == nil {
		// å¦‚æœæ‰¾åˆ°äº†ä¸Šä¸€ä¸ªåŒºå—ï¼Œæ£€æŸ¥ Hash é“¾
		if lastBlock.Hash != block.ParentHash().Hex() {
			LogReorgDetected(blockNum.String(), lastBlock.Hash, block.ParentHash().Hex())
			// åªè¿”å›é”™è¯¯ï¼Œä¸åœ¨å½“å‰äº‹åŠ¡å†…åˆ é™¤ï¼ˆé¿å…è¢« defer tx.Rollback() å›æ»šï¼‰
			// ä¸Šå±‚ä¼šç»Ÿä¸€å¤„ç†å›æ»šä¸é‡æ–°è°ƒåº¦
			return ReorgError{At: new(big.Int).Set(blockNum)}
		}
	} else if err != sql.ErrNoRows {
		// æ•°æ®åº“æŸ¥è¯¢é”™è¯¯ï¼ˆä¸æ˜¯ç©ºç»“æœï¼‰
		return fmt.Errorf("failed to query parent block: %w", err)
	}
	// å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªåŒºå—æˆ–çˆ¶å—ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯åŒæ­¥å¼€å§‹ï¼‰ï¼Œç»§ç»­å¤„ç†

	// 2. å†™å…¥ Block
	_, err = tx.NamedExecContext(ctx, `
		INSERT INTO blocks (number, hash, parent_hash, timestamp)
		VALUES (:number, :hash, :parent_hash, :timestamp)
		ON CONFLICT (number) DO UPDATE SET
			hash = EXCLUDED.hash,
			parent_hash = EXCLUDED.parent_hash,
			timestamp = EXCLUDED.timestamp,
			processed_at = NOW()
	`, models.Block{
		Number:     models.BigInt{Int: blockNum},
		Hash:       block.Hash().Hex(),
		ParentHash: block.ParentHash().Hex(),
		Timestamp:  block.Time(),
	})
	if err != nil {
		return fmt.Errorf("failed to insert block: %w", err)
	}

	// 3. å¤„ç† Transfer äº‹ä»¶ï¼ˆå¦‚æœæ—¥å¿—ä¸­æœ‰ï¼‰
	for _, vLog := range data.Logs {
		transfer := p.ExtractTransfer(vLog)
		if transfer != nil {
			_, err = tx.NamedExecContext(ctx, `
				INSERT INTO transfers 
				(block_number, tx_hash, log_index, from_address, to_address, amount, token_address)
				VALUES 
				(:block_number, :tx_hash, :log_index, :from_address, :to_address, :amount, :token_address)
				ON CONFLICT (block_number, log_index) DO UPDATE SET
					from_address = EXCLUDED.from_address,
					to_address = EXCLUDED.to_address,
					amount = EXCLUDED.amount,
					token_address = EXCLUDED.token_address
			`, transfer)
			if err != nil {
				return fmt.Errorf("failed to insert transfer at block %s: %w", blockNum.String(), err)
			}
		}
	}

	// 4. æ›´æ–° Checkpointï¼ˆåœ¨åŒä¸€äº‹åŠ¡ä¸­ä¿è¯åŸå­æ€§ï¼‰
	if err := p.updateCheckpointInTx(ctx, tx, 1, blockNum); err != nil {
		return fmt.Errorf("failed to update checkpoint for block %s: %w", blockNum.String(), err)
	}

	// 5. æäº¤äº‹åŠ¡
	if err := tx.Commit(); err != nil {
		return fmt.Errorf("failed to commit transaction for block %s: %w", blockNum.String(), err)
	}
	
	// è®°å½•å¤„ç†è€—æ—¶å’Œå½“å‰åŒæ­¥é«˜åº¦
	if p.metrics != nil {
		p.metrics.RecordBlockProcessed(time.Since(start))
		// æ›´æ–°å½“å‰åŒæ­¥é«˜åº¦ gaugeï¼ˆç”¨äºç›‘æ§ï¼‰
		p.metrics.UpdateCurrentSyncHeight(blockNum.Int64())
	}
	
	return nil
}

// updateCheckpointInTx åœ¨äº‹åŠ¡å†…æ›´æ–° checkpointï¼ˆä¿è¯åŸå­æ€§ï¼‰
func (p *Processor) updateCheckpointInTx(ctx context.Context, tx *sqlx.Tx, chainID int64, blockNumber *big.Int) error {
	_, err := tx.ExecContext(ctx, `
		INSERT INTO sync_checkpoints (chain_id, last_synced_block)
		VALUES ($1, $2)
		ON CONFLICT (chain_id) DO UPDATE SET 
			last_synced_block = EXCLUDED.last_synced_block,
			updated_at = NOW()
	`, chainID, blockNumber.String())
	
	if err != nil {
		return fmt.Errorf("failed to update checkpoint: %w", err)
	}
	
	return nil
}

// UpdateCheckpoint æ›´æ–°åŒæ­¥æ£€æŸ¥ç‚¹ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
// è­¦å‘Šï¼šæ­¤æ–¹æ³•åœ¨äº‹åŠ¡å¤–è°ƒç”¨ï¼Œå­˜åœ¨æ•°æ®ä¸ä¸€è‡´é£é™©ï¼Œå»ºè®®ç»Ÿä¸€ä½¿ç”¨äº‹åŠ¡å†…æ›´æ–°

// ExtractTransfer ä»åŒºå—æ—¥å¿—ä¸­æå– ERC20 Transfer äº‹ä»¶
func (p *Processor) ExtractTransfer(vLog types.Log) *models.Transfer {
	// æ£€æŸ¥æ˜¯å¦ä¸º Transfer äº‹ä»¶ (topic[0])
	if len(vLog.Topics) < 3 || vLog.Topics[0] != TransferEventHash {
		return nil
	}

	from := common.BytesToAddress(vLog.Topics[1].Bytes())
	to := common.BytesToAddress(vLog.Topics[2].Bytes())
	// ä½¿ç”¨ uint256 å¤„ç†é‡‘é¢ï¼Œä¿è¯é‡‘èçº§ç²¾åº¦
	amount := models.NewUint256FromBigInt(new(big.Int).SetBytes(vLog.Data))

	return &models.Transfer{
		BlockNumber:  models.BigInt{Int: new(big.Int).SetUint64(vLog.BlockNumber)},
		TxHash:       vLog.TxHash.Hex(),
		LogIndex:     uint(vLog.Index),
		From:         strings.ToLower(from.Hex()),
		To:           strings.ToLower(to.Hex()),
		Amount:       amount,
		TokenAddress: strings.ToLower(vLog.Address.Hex()),
	}
}

// ProcessBatch æ‰¹é‡å¤„ç†å¤šä¸ªåŒºå—ï¼ˆç”¨äºå†å²æ•°æ®åŒæ­¥ä¼˜åŒ–ï¼‰
func (p *Processor) ProcessBatch(ctx context.Context, blocks []BlockData, chainID int64) error {
	if len(blocks) == 0 {
		return nil
	}
	
	// æ”¶é›†æœ‰æ•ˆçš„ blocks å’Œ transfers
	validBlocks := []models.Block{}
	validTransfers := []models.Transfer{}
	
	for _, data := range blocks {
		if data.Err != nil {
			continue
		}
		
		block := data.Block
		validBlocks = append(validBlocks, models.Block{
			Number:     models.BigInt{Int: block.Number()},
			Hash:       block.Hash().Hex(),
			ParentHash: block.ParentHash().Hex(),
			Timestamp:  block.Time(),
		})
		
		// å¤„ç† transfers
		for _, vLog := range data.Logs {
			transfer := p.ExtractTransfer(vLog)
			if transfer != nil {
				validTransfers = append(validTransfers, *transfer)
			}
		}
	}
	
	if len(validBlocks) == 0 {
		return nil
	}
	
	// ä½¿ç”¨ BulkInserter è¿›è¡Œé«˜æ•ˆæ‰¹é‡å†™å…¥ï¼ˆCOPY æˆ– UNNESTï¼‰
	inserter := NewBulkInserter(p.db)
	
	// æ‰¹é‡æ’å…¥ blocks
	if err := inserter.InsertBlocksBatch(ctx, validBlocks); err != nil {
		return fmt.Errorf("batch insert blocks failed: %w", err)
	}
	
	// æ‰¹é‡æ’å…¥ transfers
	if len(validTransfers) > 0 {
		if err := inserter.InsertTransfersBatch(ctx, validTransfers); err != nil {
			return fmt.Errorf("batch insert transfers failed: %w", err)
		}
	}
	
	// æ›´æ–° checkpoint åˆ°æœ€åä¸€ä¸ªåŒºå—
	lastBlock := blocks[len(blocks)-1].Block
	_, err := p.db.ExecContext(ctx, `
		INSERT INTO sync_checkpoints (chain_id, last_synced_block)
		VALUES ($1, $2)
		ON CONFLICT (chain_id) DO UPDATE SET 
			last_synced_block = EXCLUDED.last_synced_block,
			updated_at = NOW()
	`, chainID, lastBlock.Number().String())
	if err != nil {
		return fmt.Errorf("batch checkpoint update failed: %w", err)
	}
	
	return nil
}

// FindCommonAncestor é€’å½’æŸ¥æ‰¾å…±åŒç¥–å…ˆï¼ˆå¤„ç†æ·±åº¦é‡ç»„ï¼‰
// è¿”å›å…±åŒç¥–å…ˆçš„åŒºå—å·å’Œå“ˆå¸Œï¼Œä»¥åŠéœ€è¦åˆ é™¤çš„åŒºå—åˆ—è¡¨
func (p *Processor) FindCommonAncestor(ctx context.Context, blockNum *big.Int) (*big.Int, string, []*big.Int, error) {
	log.Printf("ğŸ” Finding common ancestor from block %s", blockNum.String())
	
	toDelete := []*big.Int{}
	currentNum := new(big.Int).Set(blockNum)
	maxLookback := big.NewInt(1000) // æœ€å¤§å›é€€1000ä¸ªå—é˜²æ­¢æ— é™å¾ªç¯
	
	for currentNum.Cmp(big.NewInt(0)) > 0 && new(big.Int).Sub(blockNum, currentNum).Cmp(maxLookback) <= 0 {
		// ä»RPCè·å–é“¾ä¸ŠåŒºå—
		rpcBlock, err := p.client.BlockByNumber(ctx, currentNum)
		if err != nil {
			return nil, "", nil, fmt.Errorf("failed to get block %s from RPC: %w", currentNum.String(), err)
		}
		
		// æŸ¥è¯¢æœ¬åœ°æ•°æ®åº“ä¸­ç›¸åŒé«˜åº¦çš„åŒºå—
		var localBlock models.Block
		err = p.db.GetContext(ctx, &localBlock, 
			"SELECT hash FROM blocks WHERE number = $1", currentNum.String())
		
		if err == sql.ErrNoRows {
			// æœ¬åœ°æ²¡æœ‰è¿™ä¸ªåŒºå—ï¼Œç»§ç»­å¾€å‰æ‰¾
			toDelete = append(toDelete, new(big.Int).Set(currentNum))
			currentNum.Sub(currentNum, big.NewInt(1))
			continue
		}
		if err != nil {
			return nil, "", nil, fmt.Errorf("database error at block %s: %w", currentNum.String(), err)
		}
		
		// æ£€æŸ¥å“ˆå¸Œæ˜¯å¦åŒ¹é…
		if strings.ToLower(localBlock.Hash) == strings.ToLower(rpcBlock.Hash().Hex()) {
			// æ‰¾åˆ°å…±åŒç¥–å…ˆï¼
			log.Printf("âœ… Common ancestor found at block %s (hash: %s)", 
				currentNum.String(), localBlock.Hash)
			return currentNum, localBlock.Hash, toDelete, nil
		}
		
		// å“ˆå¸Œä¸åŒ¹é…ï¼Œè¿™ä¸ªåŒºå—ä¹Ÿåœ¨é‡ç»„é“¾ä¸Šï¼Œéœ€è¦åˆ é™¤
		toDelete = append(toDelete, new(big.Int).Set(currentNum))
		
		// ç»§ç»­æŸ¥æ‰¾çˆ¶åŒºå—ï¼ˆä½¿ç”¨RPCè¿”å›çš„parent hashï¼‰
		parentNum := new(big.Int).Sub(currentNum, big.NewInt(1))
		currentNum.Set(parentNum)
	}
	
	return nil, "", nil, fmt.Errorf("common ancestor not found within %s blocks", maxLookback.String())
}

// HandleDeepReorg å¤„ç†æ·±åº¦é‡ç»„ï¼ˆè¶…è¿‡1ä¸ªå—çš„é‡ç»„ï¼‰
// è°ƒç”¨æ­¤å‡½æ•°å‰å¿…é¡»åœæ­¢Fetcherå¹¶æ¸…ç©ºå…¶é˜Ÿåˆ—
func (p *Processor) HandleDeepReorg(ctx context.Context, blockNum *big.Int) (*big.Int, error) {
	// æŸ¥æ‰¾å…±åŒç¥–å…ˆ
	ancestorNum, _, toDelete, err := p.FindCommonAncestor(ctx, blockNum)
	if err != nil {
		return nil, fmt.Errorf("failed to find common ancestor: %w", err)
	}
	
	LogReorgHandled(len(toDelete), ancestorNum.String())
	
	// åœ¨å•ä¸ªäº‹åŠ¡å†…æ‰§è¡Œå›æ»šï¼ˆä¿è¯åŸå­æ€§ï¼‰
	tx, err := p.db.BeginTxx(ctx, &sql.TxOptions{Isolation: sql.LevelSerializable})
	if err != nil {
		return nil, fmt.Errorf("failed to begin reorg transaction: %w", err)
	}
	defer tx.Rollback()
	
	// æ‰¹é‡åˆ é™¤æ‰€æœ‰åˆ†å‰åŒºå—ï¼ˆcascade ä¼šè‡ªåŠ¨åˆ é™¤ transfersï¼‰
	if len(toDelete) > 0 {
		// æ‰¾åˆ°æœ€å°çš„è¦åˆ é™¤çš„å—å·
		minDelete := toDelete[0]
		for _, num := range toDelete {
			if num.Cmp(minDelete) < 0 {
				minDelete = num
			}
		}
		// åˆ é™¤æ‰€æœ‰ >= minDelete çš„å—ï¼ˆæ›´é«˜æ•ˆï¼‰
		_, err := tx.ExecContext(ctx, "DELETE FROM blocks WHERE number >= $1", minDelete.String())
		if err != nil {
			return nil, fmt.Errorf("failed to delete reorg blocks: %w", err)
		}
	}
	
	// æ›´æ–° checkpoint å›é€€åˆ°ç¥–å…ˆé«˜åº¦
	_, err = tx.ExecContext(ctx, `
		INSERT INTO sync_checkpoints (chain_id, last_synced_block)
		VALUES ($1, $2)
		ON CONFLICT (chain_id) DO UPDATE SET 
			last_synced_block = EXCLUDED.last_synced_block,
			updated_at = NOW()
	`, 1, ancestorNum.String())
	if err != nil {
		return nil, fmt.Errorf("failed to update checkpoint during reorg: %w", err)
	}
	
	// æäº¤äº‹åŠ¡
	if err := tx.Commit(); err != nil {
		return nil, fmt.Errorf("failed to commit reorg transaction: %w", err)
	}
	
	log.Printf("âœ… Deep reorg handled. Safe to resume from block %s", 
		new(big.Int).Add(ancestorNum, big.NewInt(1)).String())
	
	return ancestorNum, nil
}
