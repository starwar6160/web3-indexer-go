package engine

import (
	"context"
	"fmt"
	"log/slog"
	"math/big"
	"strings"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"golang.org/x/time/rate"
)

type BlockData struct {
	Block *types.Block
	Logs  []types.Log
	Err   error
}

type Fetcher struct {
	pool        RPCClient // RPCå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒMockå’ŒçœŸå®å®ç°
	concurrency int
	jobs        chan *big.Int
	Results     chan BlockData
	limiter     *rate.Limiter // é€Ÿç‡é™åˆ¶å™¨
	stopCh      chan struct{} // ç”¨äºåœæ­¢è°ƒåº¦
	stopOnce    sync.Once     // ç¡®ä¿åªåœæ­¢ä¸€æ¬¡

	// Pause/Resume æœºåˆ¶ï¼šç”¨ sync.Cond æ›¿ä»£ channel é¿å…ç«æ€
	pauseMu   sync.Mutex
	pauseCond *sync.Cond
	paused    bool

	// Watched addresses for contract monitoring
	watchedAddresses []common.Address
}

func NewFetcher(pool RPCClient, concurrency int) *Fetcher {
	// é»˜è®¤é™åˆ¶ï¼šæ¯ç§’100ä¸ªè¯·æ±‚ï¼Œçªå‘200
	limiter := rate.NewLimiter(rate.Limit(100), 200)

	f := &Fetcher{
		pool:        pool,
		concurrency: concurrency,
		jobs:        make(chan *big.Int, concurrency*2),
		Results:     make(chan BlockData, concurrency*2),
		limiter:     limiter,
		stopCh:      make(chan struct{}),
		paused:      false,
	}
	f.pauseCond = sync.NewCond(&f.pauseMu)
	return f
}

func NewFetcherWithLimiter(pool RPCClient, concurrency int, rps int, burst int) *Fetcher {
	limiter := rate.NewLimiter(rate.Limit(rps), burst)

	f := &Fetcher{
		pool:        pool,
		concurrency: concurrency,
		jobs:        make(chan *big.Int, concurrency*2),
		Results:     make(chan BlockData, concurrency*2),
		limiter:     limiter,
		stopCh:      make(chan struct{}),
		paused:      false,
	}
	f.pauseCond = sync.NewCond(&f.pauseMu)
	return f
}

// SetWatchedAddresses sets the contract addresses to monitor for Transfer events
func (f *Fetcher) SetWatchedAddresses(addresses []string) {
	f.watchedAddresses = make([]common.Address, 0, len(addresses))
	for _, addr := range addresses {
		if addr != "" {
			f.watchedAddresses = append(f.watchedAddresses, common.HexToAddress(addr))
		}
	}
}

func (f *Fetcher) Start(ctx context.Context, wg *sync.WaitGroup) {
	Logger.Info("ğŸ“¢ [Fetcher] å¼•æ“åç¨‹å·²è¿›å…¥ Start å‡½æ•°ï¼",
		slog.Int("concurrency", f.concurrency),
	)
	for i := 0; i < f.concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			Logger.Info("ğŸŒ€ [Fetcher] å¾ªç¯æŠ“å–åç¨‹æ­£å¼å¯åŠ¨...",
				slog.Int("worker_id", workerID),
			)
			f.worker(ctx, wg)
		}(i)
	}
}

func (f *Fetcher) worker(ctx context.Context, wg *sync.WaitGroup) {
	defer wg.Done()

	for {
		select {
		case <-ctx.Done():
			return
		case <-f.stopCh:
			return
		case bn, ok := <-f.jobs:
			if !ok {
				return
			}

			// æ£€æŸ¥æ˜¯å¦æš‚åœï¼ˆReorg å¤„ç†æœŸé—´ï¼‰
			f.pauseMu.Lock()
			for f.paused {
				// ç­‰å¾…æ¢å¤ä¿¡å·ï¼ˆä½¿ç”¨ Cond.Wait é¿å…ç«æ€ï¼‰
				f.pauseCond.Wait()
			}
			f.pauseMu.Unlock()

			// æ£€æŸ¥æ˜¯å¦å·²åœæ­¢ï¼ˆåœ¨ unlock åå†æ£€æŸ¥ï¼‰
			select {
			case <-ctx.Done():
				return
			case <-f.stopCh:
				return
			default:
			}

			// ç­‰å¾…é€Ÿç‡é™åˆ¶ä»¤ç‰Œ
			if err := f.limiter.Wait(ctx); err != nil {
				select {
				case f.Results <- BlockData{Err: err}:
				case <-ctx.Done():
					return
				case <-f.stopCh:
					return
				}
				continue
			}

			// è·å–åŒºå—æ•°æ®
			block, logs, err := f.fetchBlockWithLogs(ctx, bn)

			select {
			case f.Results <- BlockData{Block: block, Logs: logs, Err: err}:
			case <-ctx.Done():
				return
			case <-f.stopCh:
				return
			}
		}
	}
}

func (f *Fetcher) fetchBlockWithLogs(ctx context.Context, bn *big.Int) (*types.Block, []types.Log, error) {
	var block *types.Block
	var err error
	start := time.Now()

	// æŒ‡æ•°é€€é¿é‡è¯•é€»è¾‘ (RPC pool å†…éƒ¨æœ‰èŠ‚ç‚¹æ•…éšœè½¬ç§»)
	for retries := 0; retries < 3; retries++ {
		block, err = f.pool.BlockByNumber(ctx, bn)
		if err == nil {
			break
		}

		// æ ¹æ®é”™è¯¯ç±»å‹é€‰æ‹©é€€é¿æ—¶é—´
		// 429 (Too Many Requests) éœ€è¦æ›´é•¿çš„é€€é¿
		var backoff time.Duration
		if strings.Contains(err.Error(), "429") || strings.Contains(err.Error(), "too many requests") {
			// 429 é”™è¯¯ï¼š1s, 2s, 4sï¼ˆæ›´é•¿çš„é€€é¿ï¼‰
			backoff = time.Duration(1000*(1<<retries)) * time.Millisecond
		} else {
			// å…¶ä»–é”™è¯¯ï¼š100ms, 200ms, 400ms
			backoff = time.Duration(100*(1<<retries)) * time.Millisecond
		}

		LogRPCRetry("BlockByNumber", retries+1, err)
		select {
		case <-time.After(backoff):
			// ç»§ç»­é‡è¯•
		case <-ctx.Done():
			return nil, nil, ctx.Err()
		case <-f.stopCh:
			return nil, nil, fmt.Errorf("fetcher stopped")
		}
	}

	if err != nil {
		return nil, nil, err
	}

	// è·å–è¯¥åŒºå—çš„æ—¥å¿—ï¼ˆTransferäº‹ä»¶ï¼‰
	// å¦‚æœæœ‰ç›‘å¬çš„åœ°å€ï¼Œåªè·å–è¿™äº›åœ°å€çš„æ—¥å¿—ï¼›å¦åˆ™è·å–æ‰€æœ‰Transferäº‹ä»¶
	filterQuery := ethereum.FilterQuery{
		FromBlock: bn,
		ToBlock:   bn,
		Topics:    [][]common.Hash{{TransferEventHash}},
	}

	if len(f.watchedAddresses) > 0 {
		filterQuery.Addresses = f.watchedAddresses
		Logger.Debug("fetcher_filtering_logs",
			slog.String("block", bn.String()),
			slog.Int("watched_addresses_count", len(f.watchedAddresses)),
		)
	}

	logs, err := f.pool.FilterLogs(ctx, filterQuery)
	if err != nil {
		// æ—¥å¿—è·å–å¤±è´¥ä¸é˜»å¡åŒºå—å¤„ç†ï¼Œä½†è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
		Logger.Warn("logs_fetch_failed",
			slog.String("block_number", bn.String()),
			slog.String("error", err.Error()),
			slog.String("action", "continuing_with_empty_logs"),
		)
		logs = []types.Log{}
	}

	// è®°å½• fetch è€—æ—¶
	GetMetrics().RecordFetcherJobCompleted(time.Since(start))

	return block, logs, nil
}

func (f *Fetcher) Schedule(ctx context.Context, start, end *big.Int) error {
	// QuickNode ä¼˜åŒ–ï¼šeth_getLogs å•æ¬¡æŸ¥è¯¢æœ€å¤š 2000 ä¸ªå—
	// å¦‚æœèŒƒå›´è¶…è¿‡ 2000ï¼Œè‡ªåŠ¨åˆ†æ‰¹å¤„ç†
	maxBlockRange := big.NewInt(2000)
	current := new(big.Int).Set(start)

	for current.Cmp(end) <= 0 {
		batchEnd := new(big.Int).Add(current, maxBlockRange)
		if batchEnd.Cmp(end) > 0 {
			batchEnd = new(big.Int).Set(end)
		}

		// è°ƒåº¦å½“å‰æ‰¹æ¬¡çš„å—
		for i := new(big.Int).Set(current); i.Cmp(batchEnd) <= 0; i.Add(i, big.NewInt(1)) {
			bn := new(big.Int).Set(i)

			select {
			case <-ctx.Done():
				return ctx.Err()
			case <-f.stopCh:
				return fmt.Errorf("fetcher stopped")
			case f.jobs <- bn:
			}
		}

		// ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹
		current = new(big.Int).Add(batchEnd, big.NewInt(1))
	}
	return nil
}

// Stop ä¼˜é›…åœ°åœæ­¢ Fetcherï¼Œæ¸…ç©ºä»»åŠ¡é˜Ÿåˆ—
func (f *Fetcher) Stop() {
	f.stopOnce.Do(func() {
		close(f.stopCh)
		// æ¸…ç©º jobs channel é˜²æ­¢é˜»å¡
		go func() {
			for range f.jobs {
			}
		}()
	})
}

// Pause æš‚åœ Fetcherï¼ˆç”¨äº Reorg å¤„ç†æœŸé—´é˜²æ­¢å†™å…¥æ—§åˆ†å‰æ•°æ®ï¼‰
func (f *Fetcher) Pause() {
	f.pauseMu.Lock()
	defer f.pauseMu.Unlock()
	if !f.paused {
		f.paused = true
		LogFetcherPaused("reorg_handling")
	}
}

// Resume æ¢å¤ Fetcher
func (f *Fetcher) Resume() {
	f.pauseMu.Lock()
	defer f.pauseMu.Unlock()
	if f.paused {
		f.paused = false
		f.pauseCond.Broadcast() // å”¤é†’æ‰€æœ‰ç­‰å¾…çš„ worker
		LogFetcherResumed()
	}
}

// IsPaused è¿”å›å½“å‰æ˜¯å¦æš‚åœ
func (f *Fetcher) IsPaused() bool {
	f.pauseMu.Lock()
	defer f.pauseMu.Unlock()
	return f.paused
}
func (f *Fetcher) SetRateLimit(rps int, burst int) {
	f.limiter.SetLimit(rate.Limit(rps))
	f.limiter.SetBurst(burst)
}
