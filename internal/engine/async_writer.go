package engine

import (
	"context"
	"database/sql"
	"log/slog"
	"sync"
	"sync/atomic"
	"time"

	"web3-indexer-go/internal/models"

	"github.com/jmoiron/sqlx"
)

// ğŸ”¥ å·¥ä¸šçº§å¼‚æ­¥å†™å…¥å™¨ (AsyncWriter) - æ¨ªæ»¨å®éªŒå®¤ä¸“ç”¨ç‰ˆ
// é’ˆå¯¹ AMD 3800X + 128G RAM + 990 PRO æè‡´ä¼˜åŒ–
// æ ¸å¿ƒç­–ç•¥ï¼šæµ·é‡å†…å­˜ç¼“å†² + æ‰¹é‡äº‹åŠ¡ + ç©ºå—è¿‡æ»¤

// PersistTask æºå¸¦éœ€è¦è½ç›˜çš„åŸå§‹äº¤æ˜“æ•°æ®
type PersistTask struct {
	Height    uint64            // åŒºå—é«˜åº¦
	Block     models.Block      // åŒºå—å…ƒæ•°æ®
	Transfers []models.Transfer // æå–å‡ºçš„è½¬è´¦è®°å½•
	Sequence  uint64            // æ¶ˆæ¯åºåˆ—å· (ç”¨äºè¿½è¸ª)
}

// AsyncWriter è´Ÿè´£å¼‚æ­¥æŒä¹…åŒ–é€»è¾‘
type AsyncWriter struct {
	// 1. è¾“å…¥é€šé“ï¼š100,000 æ·±åº¦ç¼“å†²ï¼Œåˆ©ç”¨ 128G å†…å­˜å½»åº•æ¶ˆé™¤èƒŒå‹
	taskChan chan PersistTask

	db            *sqlx.DB
	orchestrator  *Orchestrator
	ephemeralMode bool // ğŸ”¥ æ–°å¢ï¼šæ˜¯å¦ä¸ºå…¨å†…å­˜æ¨¡å¼

	// 2. æ‰¹å¤„ç†é…ç½®
	batchSize     int
	flushInterval time.Duration

	// çŠ¶æ€æ§åˆ¶
	ctx    context.Context
	cancel context.CancelFunc
	wg     sync.WaitGroup

	// æ€§èƒ½æŒ‡æ ‡ (åŸå­æ“ä½œ)
	diskWatermark atomic.Uint64
	writeDuration atomic.Int64 // çº³ç§’
}

// NewAsyncWriter åˆå§‹åŒ–
func NewAsyncWriter(db *sqlx.DB, o *Orchestrator, ephemeral bool) *AsyncWriter {
	ctx, cancel := context.WithCancel(context.Background())
	w := &AsyncWriter{
		// ğŸš€ 16G RAM è°ƒä¼˜ï¼šæå‡è‡³ 15,000ï¼Œç»™äºˆæ¶ˆè´¹ç«¯æ›´å¤šç¼“å†²ç©ºé—´
		taskChan:      make(chan PersistTask, 15000),
		db:            db,
		orchestrator:  o,
		ephemeralMode: ephemeral,
		batchSize:     200, // ğŸš€ 16G RAM è°ƒä¼˜ï¼šç¼©å°æ‰¹æ¬¡ï¼Œå‡å°‘å¤§äº‹åŠ¡å¯¹ I/O çš„ç‹¬å 
		flushInterval: 500 * time.Millisecond,
		ctx:           ctx,
		cancel:        cancel,
	}
	return w
}

// Start å¯åŠ¨å†™å…¥ä¸»å¾ªç¯
func (w *AsyncWriter) Start() {
	slog.Info("ğŸ“ AsyncWriter: Engine Started",
		"buffer_cap", cap(w.taskChan),
		"batch_size", w.batchSize,
		"flush_interval", w.flushInterval)

	w.wg.Add(1)
	go w.run()
}

func (w *AsyncWriter) run() {
	defer w.wg.Done()

	batch := make([]PersistTask, 0, w.batchSize)
	ticker := time.NewTicker(w.flushInterval)
	defer ticker.Stop()

	for {
		select {
		case <-w.ctx.Done():
			// ä¼˜é›…é€€å‡ºï¼šå¤„ç†å‰©ä½™ä»»åŠ¡
			if len(batch) > 0 {
				w.flush(batch)
			}
			return

		case task := <-w.taskChan:
			// ğŸš€ ç´§æ€¥æ³„å‹é˜€ï¼šå¦‚æœå †ç§¯è¶…è¿‡ 90%ï¼Œè§¦å‘ä¸¢å¼ƒæ¨¡å¼
			if len(w.taskChan) > cap(w.taskChan)*90/100 {
				w.emergencyDrain()
				batch = batch[:0] // æ¸…ç©ºå½“å‰æ‰¹æ¬¡ï¼Œä»æ³„å‹åçš„ç‚¹é‡æ–°å¼€å§‹
				continue
			}

			batch = append(batch, task)
			if len(batch) >= w.batchSize {
				w.flush(batch)
				batch = batch[:0]
			}

		case <-ticker.C:
			if len(batch) > 0 {
				w.flush(batch)
				batch = batch[:0]
			}
		}
	}
}

// flush æ‰§è¡Œæ‰¹é‡å†™å…¥äº‹åŠ¡
func (w *AsyncWriter) flush(batch []PersistTask) {
	if len(batch) == 0 {
		return
	}

	start := time.Now()

	// ğŸš€ ğŸ”¥ Ephemeral Mode (å†…å­˜é»‘æ´æ¨¡å¼)
	if w.ephemeralMode {
		maxHeight := uint64(0)
		totalEvents := 0
		for _, task := range batch {
			if task.Height > maxHeight {
				maxHeight = task.Height
			}
			totalEvents += len(task.Transfers)
			GetMetrics().RecordBlockActivity(1)
		}
		
		// ä»ç„¶æ›´æ–°å†…å­˜æ°´ä½çº¿å’Œè§†è§‰è¿›åº¦
		w.diskWatermark.Store(maxHeight)
		w.orchestrator.AdvanceDBCursor(maxHeight)
		w.orchestrator.DispatchLog("INFO", "ğŸ”¥ Ephemeral Flush: Metadata Ignored", "height", maxHeight, "dropped_events", totalEvents)
		return
	}

	// å¼€å¯é«˜æ€§èƒ½äº‹åŠ¡
	tx, err := w.db.BeginTxx(w.ctx, nil)
	if err != nil {
		slog.Error("ğŸ“ AsyncWriter: BeginTx failed", "err", err)
		return
	}
	defer func() {
		if rollbackErr := tx.Rollback(); rollbackErr != nil && rollbackErr != sql.ErrTxDone {
			slog.Debug("ğŸ“ AsyncWriter: Rollback skipped", "reason", "already_committed")
		}
	}()

	var (
		maxHeight         uint64 = 0
		totalTransfers           = 0
		validBlocks              = 0
		blocksToInsert    []models.Block
		transfersToInsert []models.Transfer
	)

	for _, task := range batch {
		if task.Height > maxHeight {
			maxHeight = task.Height
		}

		// ğŸš€ å³ä½¿æ˜¯ç©ºå—ï¼Œä¹Ÿè®°å½•å¤„ç†æ´»åŠ¨ï¼Œç¡®ä¿ BPS æŒ‡æ ‡çœŸå®åæ˜ åŒæ­¥é€Ÿåº¦
		GetMetrics().RecordBlockActivity(1)

		// âœ… å¿…é¡»å§‹ç»ˆå†™å…¥åŒºå—å…ƒæ•°æ®ï¼ˆå³ä½¿ç©ºå—ï¼‰
		// å¦åˆ™ /api/blocks ä¼šé•¿æœŸåœç•™åœ¨æ—§é«˜åº¦ï¼Œé€ æˆ UI ä¸é“¾ä¸Šé«˜åº¦ä¸¥é‡ä¸ä¸€è‡´ã€‚
		validBlocks++
		blocksToInsert = append(blocksToInsert, task.Block)

		if len(task.Transfers) > 0 {
			totalTransfers += len(task.Transfers)
			transfersToInsert = append(transfersToInsert, task.Transfers...)
		}
	}

	if validBlocks > 0 {
		// ğŸš€ ä½¿ç”¨ BulkInserter (COPY åè®®) è¿›è¡Œç‰©ç†è½ç›˜
		inserter := NewBulkInserter(w.db)

		// 1. æ‰¹é‡å†™å…¥åŒºå—
		if err := inserter.InsertBlocksBatchTx(w.ctx, tx, blocksToInsert); err != nil {
			slog.Error("ğŸ“ AsyncWriter: Bulk insert blocks failed", "err", err)
			return
		}

		// 2. æ‰¹é‡å†™å…¥è½¬è´¦ï¼ˆæœ‰æ•°æ®æ—¶æ‰å†™ï¼‰
		if len(transfersToInsert) > 0 {
			if err := inserter.InsertTransfersBatchTx(w.ctx, tx, transfersToInsert); err != nil {
				slog.Error("ğŸ“ AsyncWriter: Bulk insert transfers failed", "err", err)
				return
			}
		}
	}

	// 3. æ›´æ–°åŒæ­¥æ£€æŸ¥ç‚¹ (SSOT ç‰©ç†ç¡®è®¤)
	if _, err := tx.ExecContext(w.ctx,
		`INSERT INTO sync_checkpoints (chain_id, last_synced_block)
		 VALUES (1, $1)
		 ON CONFLICT (chain_id) DO UPDATE SET
			last_synced_block = EXCLUDED.last_synced_block,
			updated_at = NOW()`,
		maxHeight); err != nil {
		slog.Error("ğŸ“ AsyncWriter: Update checkpoint failed", "err", err)
		return
	}

	// ğŸš€ Grafana å¯¹é½ï¼šæ›´æ–° sync_status è¡¨
	syncedBlock := int64(maxHeight & 0x7FFFFFFFFFFFFFFF) // ğŸš€ G115 å®‰å…¨æˆªæ–­

	if _, err := tx.ExecContext(w.ctx, `
		INSERT INTO sync_status (chain_id, last_processed_block, last_processed_timestamp, status)
		VALUES ($1, $2, NOW(), 'syncing')
		ON CONFLICT (chain_id) DO UPDATE SET
			last_processed_block = EXCLUDED.last_processed_block,
			last_processed_timestamp = NOW(),
			status = EXCLUDED.status
	`, 1, syncedBlock); err != nil {
		slog.Warn("ğŸ“ AsyncWriter: Update sync_status failed", "err", err)
	}

	if err := tx.Commit(); err != nil {
		slog.Error("ğŸ“ AsyncWriter: Commit failed", "err", err)
		return
	}

	// æ›´æ–°ç£ç›˜æ°´ä½çº¿
	w.diskWatermark.Store(maxHeight)
	w.writeDuration.Store(int64(time.Since(start)))

	// --- 4. é—­ç¯é€šçŸ¥ (SSOT) ---
	// æ— è®ºæ˜¯å¦å†™å…¥äº†æ•°æ®åº“ï¼ˆç©ºå—ä¹Ÿç®—åŒæ­¥æˆåŠŸï¼‰ï¼Œéƒ½è¦é€šçŸ¥ Orchestrator
	// åªæœ‰æ”¶åˆ° CmdCommitDiskï¼ŒSyncedCursor æ‰ä¼šçœŸæ­£æ¨è¿›
	w.orchestrator.Dispatch(CmdCommitDisk, maxHeight)

	// æ€§èƒ½æ—¥å¿—
	dur := time.Since(start)
	if dur > 500*time.Millisecond {
		slog.Warn("ğŸ“ AsyncWriter: SLOW WRITE DETECTED",
			"batch_len", len(batch),
			"valid_blocks", validBlocks,
			"tip", maxHeight,
			"dur", dur)
	}

	if validBlocks > 0 || dur > 100*time.Millisecond {
		slog.Info("ğŸ“ AsyncWriter: Batch Flushed",
			"batch_len", len(batch),
			"valid_blocks", validBlocks,
			"transfers", totalTransfers,
			"tip", maxHeight,
			"dur", dur)

		w.orchestrator.DispatchLog("SUCCESS", "ğŸ’¾ Batch Flushed to Disk",
			"blocks", len(batch),
			"transfers", totalTransfers,
			"tip", maxHeight)
	}
}

// Enqueue æäº¤æŒä¹…åŒ–ä»»åŠ¡ (éé˜»å¡)
func (w *AsyncWriter) Enqueue(task PersistTask) error {
	select {
	case w.taskChan <- task:
		return nil
	default:
		return sql.ErrConnDone // ç®€å•è¡¨ç¤ºé˜Ÿåˆ—æ»¡ (å®é™…ä¸åº”å‘ç”Ÿ)
	}
}

// emergencyDrain ç´§æ€¥æ³„å‹ï¼šå¿«é€Ÿæ¶ˆè€— Channelï¼Œåªä¿ç•™é«˜åº¦ï¼Œä¸¢å¼ƒ Metadata
func (w *AsyncWriter) emergencyDrain() {
	depth := len(w.taskChan)
	capacity := cap(w.taskChan)
	slog.Warn("ğŸš¨ BACKPRESSURE_CRITICAL: Initiating Emergency Drain",
		"depth", depth,
		"capacity", capacity)

	// é€šçŸ¥å¤§è„‘ï¼šè¿›å…¥å‹åŠ›æ³„å‹æ¨¡å¼
	w.orchestrator.SetSystemState(SystemStateDegraded)

	count := 0
	var lastHeight uint64

	// æ³„å‹å¾ªç¯ï¼šå¿«é€Ÿæ’ç©ºåˆ° 50%
	targetDepth := capacity * 50 / 100
	for len(w.taskChan) > targetDepth {
		select {
		case task := <-w.taskChan:
			count++
			if task.Height > lastHeight {
				lastHeight = task.Height
			}
			// ğŸš€ è®°å½•å—å¤„ç†æ´»åŠ¨ï¼Œå³ä½¿ä¸¢å¼ƒäº† Metadataï¼Œä¹Ÿç®—åŒæ­¥äº†åŒºå—
			GetMetrics().RecordBlockActivity(1)
			// ğŸš€ æ ¸å¿ƒåŠ¨ä½œï¼šä¸¢å¼ƒ Metadata (ä¸å†™åº“)
		default:
			goto done
		}
	}

done:
	// æœ€ç»ˆåŒæ­¥ä¸€æ¬¡æ¸¸æ ‡åˆ°å¤§è„‘ï¼Œè®© UI çš„ Synced æ•°å­—ç¬é—´è·³è·ƒ
	if lastHeight > 0 {
		w.orchestrator.AdvanceDBCursor(lastHeight)
	}

	slog.Info("âœ… Relief Valve Closed",
		"dropped_blocks", count,
		"new_synced_tip", lastHeight)

	// æ¢å¤çŠ¶æ€ (å¦‚æœåç»­å¹³ç¨³ï¼ŒOrchestrator ä¹Ÿä¼šè‡ªåŠ¨è¯„ä¼°)
	w.orchestrator.SetSystemState(SystemStateRunning)
}

// GetMetrics è·å–æ€§èƒ½æŒ‡æ ‡
func (w *AsyncWriter) GetMetrics() map[string]interface{} {
	return map[string]interface{}{
		"disk_watermark":    w.diskWatermark.Load(),
		"write_duration_ms": time.Duration(w.writeDuration.Load()).Milliseconds(),
		"queue_depth":       len(w.taskChan),
	}
}

// Shutdown ä¼˜é›…å…³é—­
func (w *AsyncWriter) Shutdown(timeout time.Duration) error {
	slog.Info("ğŸ“ AsyncWriter: Shutting down...")
	w.cancel()

	done := make(chan struct{})
	go func() {
		w.wg.Wait()
		close(done)
	}()

	select {
	case <-done:
		return nil
	case <-time.After(timeout):
		return context.DeadlineExceeded
	}
}
